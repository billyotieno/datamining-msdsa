---
title: "Web Scrapping with R"
author: 'Author: Dickson Owuor (Ph.D.)'
date: "14/06/2022"
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    toc: yes
    number_sections: no
---


# Data Extraction
Data is increasingly becoming the lifeblood of the digital economy and as most companies transit into online businesses, the importance of data is increasing rapidly. For data to be useful, it has to be collected and transformed into a form suitable for analysis.

the process of retrieving data from data sources for further data processing or storage. In this tutorial, we use the **rvest** R library to perform Web scrapping on a Wikipedia page. Web scrapping is an example technique used for data extraction. The tutorial is adopted from:

* [CRAN rvest library](https://cran.r-project.org/web/packages/rvest/index.html)


# Installation of libraries
Install and load the **rvest** package.

```{r eval=FALSE}
install.packages("rvest")
library(rvest)

```

# Usage and documentation

```{r eval=FALSE}
??rvest

```

# Selecting URL and reading HTML content
We begin the extraction process by specifying the URL of interest and reading its contents.

```{r eval=FALSE}
wikiurl<-read_html("https://en.wikipedia.org/wiki/List_of_highest-grossing_films")

```

# Piping to obtain data of interest
In this tutorial, we are interested in scrapping *'movie charts'* tables from the URL. 

```{r eval=FALSE}
# Get the movie charts from wikipedia and pipe (%>%) and fill the table
moviecharts<-wikiurl%>%html_table(., fill=T)

```

# Identifying data and displaying it
There are multiple tables in the page, we specify the one we need and store it in a data-frame.

```{r eval=FALSE}
moviecharts[[4]]


#Put the movie chart into a data-frame
highestgrossfilms<-data.frame(moviecharts[[4]])
highestgrossfilms

#Numbers of rows and columns
dim(highestgrossfilms)

```

# Writing interesting data to file
In order to write our data into a CSV file, check your working directory and set it to your desired location. Then, we can write the data into the CSV file.

```{r eval=FALSE}
getwd()
setwd("C:/Users/OwuorJnr/Desktop") # change this section to yours

write.csv(highestgrossfilms,"gross.csv", row.names = TRUE)
```

# Exercise
This exercise is an assignment and you are required to:

a. Identify any Web page of your interest and perform all the steps above to scrap data of interest to you, and

b. store it into a data-frame and write it into a CSV file.



